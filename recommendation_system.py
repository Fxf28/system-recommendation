# -*- coding: utf-8 -*-
"""recommendation_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BkGk5js97qEjL7dDTJq47o-mLJOy3xI1

# Import Library
"""

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics.pairwise import cosine_similarity

"""Kode diatas menunjukan library apa saja yang digunakan

# Load Dataset
"""

from google.colab import files
files.upload()

"""Kode diatas berfungsi untuk memasukkan file kaggle.json."""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download vivekparasharr/recommender-system-e-commerce-dataset
!unzip recommender-system-e-commerce-dataset.zip

"""Kode diatas berfungsi untuk mengunduh dataset dari kaggle dan melakukan ekstraksi pada file yang di unduh. terdapat 4 dataset yaitu:
- context.csv
- interactions.csv
- products.csv
- reviews.csv
- users.csv

# Data Understanding
"""

context = pd.read_csv('/content/context.csv')
interactions = pd.read_csv('/content/interactions.csv')
products = pd.read_csv('/content/products.csv')
reviews = pd.read_csv('/content/reviews.csv')
users = pd.read_csv('/content/users.csv')

print("Jumlah data Kontekstual:", len(context))
print("Jumlah data Interaksi:", len(interactions))
print("Jumlah data Produk:", len(products))
print("Jumlah data Ulasan:", len(reviews))
print("Jumlah data Pengguna:", len(users))

"""# Univariate Exploratory Data Analysis

Variabel-variabel pada E-commerce dataset adalah sebagai berikut:
- **context**: Data kontekstual seperti waktu dalam sehari, perangkat yang digunakan, dan lokasi pengguna.
- **interactions**: Interaksi pengguna dengan produk, seperti melihat produk, menambahkan ke keranjang, dan melakukan pembelian.
- **products**: Informasi detail mengenai produk, termasuk kategori, harga, dan merek.
- **reviews**: Ulasan produk dari pengguna, mencakup penilaian (rating) dan masukan dalam bentuk teks.
- **users**: Data demografis pengguna, seperti usia, jenis kelamin, dan lokasi.

## Context Variabel
"""

context.head().T

context.info()

"""Berdasarkan output diatas dapat diketahui bahwa variabel context memiliki:
- 5000 baris
- 4 kolom (interaction_id, time_of_day, device, location)
- Tipe data int untuk interaction_id, kolom lainnya bertipe object

### Melihat nilai unik
"""

print('Jumlah interaction: ', len(context.interaction_id.unique()))
print('Waktu :', context.time_of_day.unique())
print('Perangkat :', context.device.unique())
print('Lokasi :', context.location.unique())

"""Berdasarkan output diatas dapat diketahui nilai unik pada waktu, perangkat dan lokasi.

### Visualisasi Data
"""

def countplot(data, x, title):
  sns.countplot(data=data, x=x, order=data[x].value_counts().index, width=0.5)
  plt.title(title)
  plt.show()

countplot(data=context, x='time_of_day', title="Interactions data by time_of_day")

"""Dari gambar diatas dapat diketahui bahwa interaksi paling banyak dilakukan pada waktu sore dan malam."""

countplot(data=context, x='device', title="Interactions data by device")

"""Dari data diatas dapat diketahui bahwa interaksi paling banyak dilakukan melalui perangkat mobile."""

countplot(data=context, x='location', title="Interactions data by location")

"""Dari gambar diatas dapat diketahui bahwa lokasi dengan interaksi paling banyak adalah San Francisco dan Los Angeles.

## Interactions Variable
"""

interactions.head().T

interactions.info()

"""Berdasarkan output diatas dapat diketahui bahwa variabel interactions memiliki:
- 5000 baris
- 4 kolom (user_id, product_id, interaction_type, timestamp)
- Tipe data int untuk id dan object untuk interaction_type dan timestamp

### Melihat nilai unik
"""

print('Jumlah user: ', len(interactions.user_id.unique()))
print('Jumlah product: ', len(interactions.product_id.unique()))
print('Tipe interaksi :', interactions.interaction_type.unique())
print('Tanggal :', interactions.timestamp.unique())

"""Berdasarkan output diatas dapat diketahui bahwa terdapat 996 pengguna unik dan 500 produk unik, dengan tiga jenis interaksi utama: view, add_to_cart, dan purchase. Data mencakup periode dari 1 Januari hingga 27 Juli 2024.

### Visualisasi Data
"""

countplot(data=interactions, x='interaction_type', title="Interactions data by interaction_type")

"""Dari gambar diatas dapat diketahui distribusi interaction_type yang lebih didominasi oleh view."""

top_products = interactions['product_id'].value_counts().nlargest(10)

sns.barplot(x=top_products.index.astype(str), y=top_products.values, width=0.6)
plt.title('Top 10 Produk dengan Interaksi Terbanyak')
plt.xlabel('Product ID')
plt.ylabel('Jumlah Interaksi')
plt.show()

"""Dari gambar diatas dapat terlihat top 10 produk yang memiliki jumlah interaksi paling banyak.

## Products Variabel
"""

products.head().T

products.describe().T

products.info()

"""Berdasarkan output diatas dapat diketahui bahwa variabel products memiliki:
- 500 baris
- 4 kolom (product_id, category, price, brand)
- Tipe data int untuk id, float untuk price dan object untuk category dan brand

### Melihat nilai unik
"""

print('Banyak data : ', len(products.product_id.unique()))
print('Kategori :', products.category.unique())
print('Merek :', products.brand.unique())

"""Berdasarkan output diatas dapat diketahui bahwa produk terdiri dari 500 produk unik yang tersebar dalam 7 kategori, yaitu Home & Kitchen, Clothing, Sports, Electronics, Beauty, Books, dan Toys. Produk-produk ini berasal dari 5 merek berbeda, seperti BrandA hingga BrandE.

### Visualisasi data
"""

def countplot_2(data, x, title, xlabel, ylabel, rotation):
  sns.countplot(data=data, x=x, order=data[x].value_counts().index, width=0.5)
  plt.title(title)
  plt.xticks(rotation=rotation)
  plt.xlabel(xlabel)
  plt.ylabel(ylabel)
  plt.tight_layout()
  plt.show()

countplot_2(data=products, x='category', title='Distribusi Produk per Kategori', xlabel='Kategori', ylabel='Jumlah Produk', rotation=60)

"""Dari gambar diatas dapat diketahui distribusi produk berdasarkan kategori, yang didominasi oleh Electronics dan Books."""

countplot_2(data=products, x='brand', title='Distribusi Produk per Merek', xlabel='Merek', ylabel='Jumlah Produk', rotation=0)

"""Dari gambar diatas dapat diketahui distribusi produk berdasarkan merek, yang dipimpin oleh BrandA dengan total 120 produk."""

sns.histplot(data=products, x='price', bins=30, kde=True)
plt.title('Distribusi Harga Produk')
plt.xlabel('Harga')
plt.ylabel('Jumlah Produk')
plt.show()

"""Dari gambar diatas dapat diketahui distribusi produk berdasarkan harga, harga tertinggi adalah $500."""

sns.boxplot(data=products, x='category', y='price', order=products['category'].value_counts().index)
plt.title('Distribusi Harga per Kategori Produk')
plt.xticks(rotation=45)
plt.show()

"""Dari gambar diatas dapat diketahui rentang harga tiap kategori produk.

## Reviews Variabel
"""

reviews.head().T

reviews.describe().T

reviews.info()

"""Berdasarkan output diatas dapat diketahui bahwa reviews variabel memiliki:
- 2000 baris
- 4 kolom (user_id, product_id, rating dan review_text)
- tipe data review_text adalah object dan lainnya bertipe int

### Melihat jumlah data berdasarkan user_id dan produk_id
"""

print('Jumlah user_id: ', len(reviews.user_id.unique()))
print('Jumlah product_id: ', len(reviews.product_id.unique()))

"""Berdasarkan output diatas dapat diketahui bahwa ulasan mencakup 864 pengguna unik yang memberikan penilaian terhadap 495 produk berbeda.

### Visualisasi Data
"""

countplot_2(data=reviews, x='rating', title='Distribusi Rating Produk', xlabel='Rating', ylabel='Jumlah Produk', rotation=0)

"""Dari gambar diatas dapat diketahui distribusi rating produk, yang dipimpin oleh rating 2 dan 4."""

avg_rating = reviews.groupby('product_id')['rating'].mean().sort_values(ascending=False).head(10)

sns.barplot(x=avg_rating.index.astype(str), y=avg_rating.values)
plt.title('Top 10 Produk dengan Rating Tertinggi')
plt.xlabel('Product ID')
plt.ylabel('Rata-rata Rating')
plt.show()

"""Dari gambar diatas dapat diketahui Rata-rata Rating per Produk (Top 10 Produk dengan Rating Tertinggi)"""

review_counts = reviews['product_id'].value_counts().nlargest(10)

sns.barplot(x=review_counts.index.astype(str), y=review_counts.values)
plt.title('Top 10 Produk dengan Jumlah Review Terbanyak')
plt.xlabel('Product ID')
plt.ylabel('Jumlah Review')
plt.show()

"""Dari gambar diatas dapat diketahui jumlah Review per Produk (Top 10 Produk Terbanyak Direview)."""

all_reviews = " ".join(reviews['review_text'].dropna())
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_reviews)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud dari Ulasan Pengguna')
plt.show()

"""Dari gambar diatas dapat diketahui Word Cloud dari Review Text, plot ini biasa digunakan untuk analisis teks.

## Users Variabel
"""

users.head().T

users.describe().T

users.info()

"""Berdasarkan output diatas dapat diketahui bahwa users:
- 1000 baris
- 4 kolom (user_id, age, gender dan location)
- tipe data int untuk id dan age, object untuk gender dan location

### Melihat nilai unik
"""

print('Jumlah gender: ', users.gender.unique())
print('Jumlah lokasi: ', users.location.unique())

"""Berdasarkan output diatas dapat diketahui bahwa pengguna mencakup informasi jenis kelamin dengan tiga kategori: Male, Female, dan Other, serta mencakup pengguna dari 5 lokasi berbeda yaitu San Francisco, Houston, Chicago, Los Angeles, dan New York.

### Visualisasi Data
"""

countplot_2(data=users, x='gender', title='Distribusi Gender Pengguna', xlabel='Gender', ylabel='Jumlah Pengguna', rotation=0)

"""Dari gambar diatas dapat diketahui distribusi gender pengguna."""

sns.histplot(data=users, x='age', bins=20, kde=True)
plt.title('Distribusi Usia Pengguna')
plt.xlabel('Usia')
plt.ylabel('Jumlah Pengguna')
plt.show()

"""Dari gambar diatas dapat diketahui distribusi usia pengguna."""

sns.boxplot(data=users, x='gender', y='age')
plt.title('Distribusi Usia Berdasarkan Gender')
plt.xlabel('Gender')
plt.ylabel('Usia')
plt.show()

"""Dari gambar diatas dapat diketahui perbandingan usia antar gender."""

top_locations = users['location'].value_counts().nlargest(3)

sns.barplot(x=top_locations.index, y=top_locations.values)
plt.title('Top 3 Lokasi dengan Jumlah Pengguna Terbanyak')
plt.xlabel('Lokasi')
plt.ylabel('Jumlah Pengguna')
plt.xticks(rotation=45)
plt.show()

"""Dari gambar diatas kita dapat melihat top lokasi dari user.

# Data Preparation

## Data Preprocessing

### Aggregasi interaksi per produk
"""

interaction_counts = (
    interactions
    .groupby(['product_id', 'interaction_type'])
    .size()
    .unstack(fill_value=0)
    .reset_index()
    .rename(columns={
        'view': 'total_views',
        'add_to_cart': 'total_add_to_cart',
        'purchase': 'total_purchases'
    })
)

interaction_counts.T

"""Kode ini menghasilkan sebuah tabel baru (interaction_counts) yang menampilkan jumlah total interaksi per produk.

### Aggregasi ulasan per produk
"""

product_reviews = (
    reviews
    .groupby('product_id')
    .agg({
        'rating': 'mean',
        'review_text': lambda x: ' '.join(x)
    })
    .reset_index()
    .rename(columns={'review_text': 'all_reviews'})
)

product_reviews.T

"""Pada tahap ini kita menambahkan rating dan reviews. Kode diatas membuat ringkasan ulasan untuk setiap produk berdasarkan dataset reviews.

### Gabungkan semua data dengan produk sebagai basis
"""

merged_products = (
    products
    .merge(interaction_counts, on='product_id', how='left')
    .merge(product_reviews, on='product_id', how='left')
)

merged_products.T

"""Pada tahap ini kita menggabungkan dua data yang telah kita gabungkan sebelumnya menjadi kesatuan data seperti diatas.

### Validasi Data Preprocessing
"""

merged_products.info()

merged_products.isnull().sum()

"""Setelah kita cek, ternyata terdapat missing values, karena jumlahnya tidak banyak hanya 1% dari jumlah data maka akan kita tangani dengan cara fill mean untuk rating dan fill " " string kosong untuk all_reviews.

## Penanganan Missing Values
"""

merged_products.isnull().sum()

"""### Isi rating dengan rata-rata"""

merged_products['rating'] = merged_products['rating'].fillna(merged_products['rating'].mean())

"""### Isi all_reviews dengan string kosong"""

merged_products['all_reviews'] = merged_products['all_reviews'].fillna('')

"""### Validasi"""

merged_products.isnull().sum()

"""Diatas adalah tahapan penanganan mising values.

##  Encoding Fitur Kategorikal

### Konversi kolom category dan brand ke bentuk numerik dengan One-Hot Encoding
"""

# Inisialisasi encoder
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_features = encoder.fit_transform(merged_products[['category', 'brand']])

# Buat DataFrame hasil encoding
encoded_df = pd.DataFrame(
    encoded_features,
    columns=encoder.get_feature_names_out(['category', 'brand'])
)

# Gabungkan dengan data utama
merged_products = pd.concat([merged_products, encoded_df], axis=1)

# Hapus kolom asli kategorikal
merged_products.drop(['category', 'brand'], axis=1, inplace=True)

"""Melakukan encoding pada fitur kategorikal, yaitu category dan brand.

## Text Processing untuk Ulasan
"""

# Inisialisasi TF-IDF
tfidf = TfidfVectorizer(max_features=50)  # Ambil 50 fitur utama
review_vectors = tfidf.fit_transform(merged_products['all_reviews'])

# Konversi ke DataFrame
review_df = pd.DataFrame(
    review_vectors.toarray(),
    columns=[f"review_{feature}" for feature in tfidf.get_feature_names_out()]
)

# Gabungkan dengan data produk
merged_products = pd.concat([merged_products, review_df], axis=1)

# Hapus kolom teks asli
merged_products.drop('all_reviews', axis=1, inplace=True)

"""Pada tahap diatas kita melakukan ekstraksi fitur pada text ulasan menggunakan TF-IDF.

## Normalisasi Fitur Numerik
"""

# Daftar fitur numerik
numerical_features = ['price', 'total_views', 'total_add_to_cart', 'total_purchases', 'rating']

# Normalisasi
scaler = StandardScaler()
merged_products[numerical_features] = scaler.fit_transform(merged_products[numerical_features])

"""Pada tahap diatas kita melakukan normalisasi pada fitur numerik menggunakan StandardScaler agar semua fitur memiliki skala yang seimbang.

## Seleksi Fitur Final
"""

final_columns = [
    'product_id',
    *encoder.get_feature_names_out(['category', 'brand']),  # Fitur one-hot encoding
    *numerical_features,                                    # Fitur numerik ternormalisasi
    *review_df.columns                                       # Fitur TF-IDF
]

# Subset data
final_data = merged_products[final_columns]

"""Pada tahap diatas kita memilih kolom yang akan digunakan untuk pemodelan.

## Validasi Data
"""

final_data.head().T

final_data.info()

final_data.describe().T

"""Kode diatas berguna untuk memvalidasi bentuk data akhir yang akan kita gunakan untuk tahapan pemodelan.

# Model Development menggunakan Content-Based Filtering

## Hitung Similarity Antar Produk
"""

# Hapus kolom product_id untuk membuat matriks fitur
item_features = final_data.drop('product_id', axis=1)
similarity_matrix = cosine_similarity(item_features)
print(similarity_matrix.shape)

"""Tahapan ini bertujuan untuk menghitung seberapa mirip satu produk dengan produk lainnya berdasarkan fitur-fitur selain product_id.

### Visualisasi Similarity Matrix
"""

plt.figure(figsize=(10, 8))
sns.heatmap(similarity_matrix[:10, :10], annot=True, cmap="YlGnBu")
plt.title("Similarity Matrix (10 Produk Pertama)")
plt.show()

"""Gambar diatas menunjukkan korelasi antar 10 produk pertama.

## Fungsi Rekomendasi Produk

### Fungsi untuk Mendapatkan Rekomendasi
"""

def get_product_recommendations(product_id, n=5):
    try:
        # Cari indeks produk
        product_index = final_data[final_data['product_id'] == product_id].index[0]

        # Ambil similarity scores untuk produk ini
        similar_scores = list(enumerate(similarity_matrix[product_index]))

        # Urutkan berdasarkan similarity score
        sorted_similar = sorted(similar_scores, key=lambda x: x[1], reverse=True)

        # Ambil top-n rekomendasi (exclude produk itu sendiri)
        top_products = sorted_similar[1:n+1]

        # Dapatkan product_id dari rekomendasi
        product_indices = [i[0] for i in top_products]
        recommended_products = final_data.iloc[product_indices]['product_id'].tolist()

        return recommended_products

    except IndexError:
        return "Product ID tidak ditemukan."

"""Fungsi get_product_recommendations digunakan untuk memberikan rekomendasi produk yang mirip berdasarkan skor kemiripan (cosine similarity). Fungsi ini mencari produk yang paling mirip dengan product_id yang diberikan, menyusun daftarnya berdasarkan skor tertinggi, lalu mengembalikan ID dari beberapa produk teratas yang paling relevan (kecuali dirinya sendiri). Jika product_id tidak ditemukan, fungsi akan mengembalikan pesan error.

### Contoh Penggunaan
"""

product_id = 1

# Dapatkan rekomendasi
recommendations = get_product_recommendations(product_id, n=3)

# Cetak hasil
print(f"Rekomendasi untuk Produk ID {product_id}: {recommendations}")

products = pd.read_csv("products.csv")
products[products["product_id"] == product_id].T

recommended_products = products[products["product_id"].isin(recommendations)]
print(f"\nDetail Rekomendasi untuk Produk ID {product_id}:")
recommended_products[["product_id", "category", "price", "brand"]].T

"""Diatas adalah contoh penggunaannya.

Tahapan ini melakukan:
- mengambil rekomendasi berbasis produk tertentu
- menampilkan produk tersebut
- menampilkan detail produk-produk yang direkomendasikan

## Evaluasi

### Persiapan Data Evaluasi

#### Split Data Interaksi (Training & Test)
"""

train_interactions, test_interactions = train_test_split(
    interactions,
    test_size=0.2,
    stratify=interactions['interaction_type'],
    random_state=42
)

"""Kode diatas berfungsi untuk membagi data menjadi traning dan testing dengan proporsi 80:20 dan menggunakan stratift interaction type yang berarti data akan dibagi dengan mempertahankan proporsi masing-masing jenis interaksi.

#### Ground Truth
"""

# Ambil produk yang dibeli di data test
ground_truth = (
    test_interactions[test_interactions['interaction_type'] == 'purchase']
    .groupby('product_id')['user_id']
    .apply(list)
    .to_dict()
)

"""Kode ini mengambil daftar user yang membeli setiap produk di data testing, lalu menyimpannya dalam format dictionary.

### Hitung Coverage
"""

def calculate_coverage(recommended_products, total_products):
    unique_recommended = len(set(recommended_products))
    return unique_recommended / total_products

"""Fungsi ini digunakan untuk menghitung coverage atau cakupan dari produk yang direkomendasikan, yang mengukur seberapa banyak produk yang direkomendasikan mencakup keseluruhan produk yang ada."""

all_recommended = []
for product_id in ground_truth.keys():
    all_recommended.extend(get_product_recommendations(product_id, n=10))

coverage = calculate_coverage(all_recommended, len(products))
print(f"Coverage: {coverage:.2f}")

"""Kode ini digunakan untuk menghitung coverage dari sistem rekomendasi berdasarkan produk yang direkomendasikan.

### Hitung Diversity
"""

def calculate_diversity(recommended_products, final_data):
    # Ambil fitur dari produk yang direkomendasikan (termasuk product_id)
    features_with_id = final_data[final_data['product_id'].isin(recommended_products)]

    # Drop kolom product_id untuk perhitungan similarity
    features = features_with_id.drop('product_id', axis=1)

    # Hitung similarity antar rekomendasi
    similarity = cosine_similarity(features)
    np.fill_diagonal(similarity, 0)  # Abaikan similarity dengan diri sendiri

    # Hitung diversity sebagai 1 - rata-rata similarity
    diversity = 1 - np.mean(similarity)
    return diversity

"""Fungsi ini digunakan untuk menghitung diversity dari produk yang direkomendasikan. Diversity mengukur variasi atau keragaman dalam produk yang direkomendasikan, dengan tujuan untuk menghindari rekomendasi yang terlalu mirip satu sama lain."""

diversity = calculate_diversity(all_recommended, final_data)
print(f"Diversity: {diversity:.2f}")

"""Hasil diversity dapat dilihat dari output diatas. Ini berarti menunjukkan bahwa produk yang direkomendasikan sangat beragam, dengan nilai lebih tinggi menunjukkan keragaman yang lebih baik.

# Model Development menggunakan Collaborative-Filtering

## Data Understanding
"""

df = pd.read_csv("interactions.csv")
df

"""Kode diatas kita Meload data reviews dan menyimpannya dalam variabel df, pada tahapan sebelumnya kita juga telah mengeksplorasinya."""

df.info()

"""Data review memiliki 5000 baris dan 4 kolom.

## Data Preparation

### Weight map pada interaction_type
"""

weight_map = {
    'view': 1,
    'add_to_cart': 3,
    'purchase': 5
}

df = df.copy()
df['weight'] = df['interaction_type'].map(weight_map)

"""Kode ini digunakan untuk memberikan berat (weight) pada setiap jenis interaksi pengguna berdasarkan tipe interaksi (seperti "view", "add_to_cart", "purchase"). Berat ini kemudian ditambahkan sebagai kolom baru dalam dataframe.

### Mengubah timestamp ke datetime
"""

df['timestamp'] = pd.to_datetime(df['timestamp'])

"""Mengubah tipe data timestamp, disini kolom timestamp tidak kita gunakan.

### Aggregate per user-product
"""

agg = (
    df
    .groupby(['user_id', 'product_id'])['weight']
    .sum()
    .reset_index()
)

"""Kode ini digunakan untuk mengagregasi data berdasarkan kombinasi user_id dan product_id, kemudian menjumlahkan nilai kolom weight untuk setiap kombinasi tersebut. Hasilnya adalah dataframe yang menunjukkan total bobot (weight) yang diberikan oleh setiap pengguna terhadap produk tertentu.

### Binarisasi: interaction=1 jika total weight >= threshold, else 0
"""

threshold = 3
agg['interaction'] = (agg['weight'] >= threshold).astype(int)

"""Kode ini digunakan untuk menambahkan kolom baru dalam dataframe agg yang menunjukkan apakah total bobot interaksi untuk setiap kombinasi user_id dan product_id melebihi batas ambang (threshold) tertentu, dalam hal ini nilai threshold adalah 3.

### Encode user_id & product_id ke idx 0..N-1
"""

user_ids    = agg['user_id'].unique()
product_ids = agg['product_id'].unique()
user_encoder    = {u:i for i, u in enumerate(user_ids)}
product_encoder = {p:i for i, p in enumerate(product_ids)}

agg['user_idx']    = agg['user_id'].map(user_encoder)
agg['product_idx'] = agg['product_id'].map(product_encoder)

num_users    = len(user_encoder)
num_products = len(product_encoder)

"""Melakukan encoding pada kolom userid dan productid

### Negative sampling (implicit feedback)
"""

N_neg = 3  # misal 3 negative per positive
user_pos = agg[agg['interaction']==1].groupby('user_idx')['product_idx'].apply(set).to_dict()
all_prods = set(range(num_products))

neg_samples = []
for u, pos_set in user_pos.items():
    # ambil negatif secara acak
    negs = np.random.choice(
        list(all_prods - pos_set),
        size=N_neg * len(pos_set),
        replace=True
    )
    neg_samples += [(u, p, 0) for p in negs]

neg_df = pd.DataFrame(neg_samples, columns=['user_idx','product_idx','interaction'])
pos_df = agg[['user_idx','product_idx','interaction']]

data = pd.concat([pos_df, neg_df], ignore_index=True).sample(frac=1, random_state=42)

"""Melakukan negative sampling. Kode ini digunakan untuk menghasilkan sampel negatif (non-interaksi) untuk setiap pengguna yang memiliki interaksi positif dengan produk. Kemudian, kode ini menggabungkan interaksi positif dan negatif ke dalam satu dataset yang dapat digunakan untuk pelatihan model rekomendasi, seperti untuk binary classification (apakah pengguna akan berinteraksi dengan produk atau tidak).

### Validasi data
"""

data

data.info()

# Mendapatkan jumlah user & product
print(num_users)
print(num_products)

# Mengubah rating menjadi nilai float
data['interaction'] = data['interaction'].values.astype(np.float32)

# Nilai minimum rating
min_interaction = min(data['interaction'])

# Nilai maksimal rating
max_interaction = max(data['interaction'])

print('Number of User: {}, Number of Product: {}, Min Interaction: {}, Max Interaction: {}'.format(
    num_users, num_products, min_interaction, max_interaction
))

"""Pada tahapan diatas dapat kita lihat data yang akan kita gunakan.

### Split Data
"""

# Mengacak dataset
data = data.sample(frac=1, random_state=42)
data

"""Mengacak data agar distribusinya menjadi random."""

# Membuat variabel x untuk mencocokkan data user dan product menjadi satu value
x = data[['user_idx', 'product_idx']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data['interaction'].apply(lambda x: (x - min_interaction) / (max_interaction - min_interaction)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Melakukan Splitting data untuk train dan validation

## Proses Training
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_products, embedding_size=32, dropout_rate=0.3, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)

        # Embedding layer with regularization
        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer='glorot_uniform',  # Xavier initialization for sigmoid activations
            embeddings_regularizer=regularizers.l2(1e-4)
        )
        self.user_bias = layers.Embedding(num_users, 1, embeddings_initializer='zeros')  # User bias

        self.product_embedding = layers.Embedding(
            input_dim=num_products,
            output_dim=embedding_size,
            embeddings_initializer='glorot_uniform',
            embeddings_regularizer=regularizers.l2(1e-4)
        )
        self.product_bias = layers.Embedding(num_products, 1, embeddings_initializer='zeros')  # Product bias

        # Dropout layer for regularization
        self.dropout = layers.Dropout(dropout_rate)

    def call(self, inputs):
        # inputs shape: (batch_size, 2) â†’ [user_id, product_id]
        user_id, product_id = inputs[:, 0], inputs[:, 1]

        # Get embeddings + apply dropout for regularization
        user_vec = self.user_embedding(user_id)
        user_vec = self.dropout(user_vec)
        user_b = self.user_bias(user_id)

        prod_vec = self.product_embedding(product_id)
        prod_vec = self.dropout(prod_vec)
        prod_b = self.product_bias(product_id)

        # Compute the dot product of the embeddings + biases
        dot_product = tf.reduce_sum(user_vec * prod_vec, axis=1, keepdims=True)
        x = dot_product + user_b + prod_b

        # Sigmoid activation for the final prediction
        return tf.nn.sigmoid(x)

"""Membuat class RecommenderNet dengan keras Model class. Kode ini mendefinisikan sebuah model rekomendasi berbasis Neural Collaborative Filtering (NCF) yang menggunakan embedding untuk representasi pengguna dan produk, serta bias untuk setiap pengguna dan produk. Model ini dirancang untuk memprediksi apakah seorang pengguna akan memberikan interaksi (misalnya, pembelian) terhadap suatu produk."""

# Model Compile
model = RecommenderNet(
    num_users=num_users,
    num_products=num_products,
    embedding_size=64,
    dropout_rate=0.5
)

# Compile Model
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]  # Added accuracy metric
)

"""Kode diatas berguna untuk mendefinisikan model neural network untuk sistem rekomendasi yang memanfaatkan embeddings untuk pengguna dan produk, dan kemudian mengompilasi model dengan pengaturan yang sesuai untuk training."""

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=False,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

"""pada kode diatas kita membuat dua callback yaitu:
- EarlyStopping: Melakukan penghentian pelatihan jika val_loss tidak membaik selama 5 epoch berturut-turut.
- ModelCheckpoint: Menyimpan model terbaik berdasarkan val_loss.
- ReduceLROnPlateau: Mengurangi learning rate ketika val_loss berhenti membaik.
"""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks=[early_stopping, model_checkpoint, reduce_lr]
)

"""## Mendapatkan Rekomendasi"""

products = pd.read_csv("products.csv")
df = pd.read_csv('interactions.csv')

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
product_got_interaction = df[df.user_id == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
product_no_interaction= products[~products['product_id'].isin(product_got_interaction.product_id.values)]['product_id']
product_no_interaction = list(
    set(product_no_interaction)
    .intersection(set(product_encoder.keys()))
)

product_no_interaction = [[product_encoder.get(x)] for x in product_no_interaction]
user_enc = user_encoder.get(user_id)
user_product_array = np.hstack(
    ([[user_enc]] * len(product_no_interaction), product_no_interaction)
)

"""Kode diatas berfungsi untuk mempersiapkan data input guna memberi rekomendasi produk ke seorang user yang belum pernah berinteraksi dengan produk-produk tersebut."""

interactions = model.predict(user_product_array).flatten()

top_interactions_indices = interactions.argsort()[-10:][::-1]
recommended_product_ids = [
    product_encoder.get(product_no_interaction[x][0]) for x in top_interactions_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Product with high interactions from user')
print('----' * 8)

top_product_user = (
    product_got_interaction.sort_values(
        by = 'interaction_type',
        ascending=False
    )
    .head(5)
    .product_id.values
)

product_df_rows = products[products['product_id'].isin(top_product_user)]
for row in product_df_rows.itertuples():
    print(row.product_id, ':', row.category, ':', row.brand)

print('----' * 8)
print('Top 10 product recommendation')
print('----' * 8)

recommended_product = products[products['product_id'].isin(recommended_product_ids)]
for row in recommended_product.itertuples():
    print(row.product_id, ':', row.category, ':', row.brand)

"""Output diatas adalah salah satu penggunaan collaborative filtering. Perhatikanlah, beberapa produk rekomendasi menyediakan kategori  yang sesuai dengan interactions user.

## Evaluasi

### Plot Training-Validation RMSE
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Dari gambar diatas dapat kita lihat Root Mean Squared Error (RMSE) antara training dan validation selama proses training. Model menunjukkan tanda-tanda overfitting, di mana loss pada training set terus menurun tetapi val_loss mulai stagnan.

### Evaluasi RMSE pada Validation Set
"""

val_loss, val_rmse = model.evaluate(x_val, y_val, batch_size=64)
print(f"Validation Loss: {val_loss:.4f}, Validation RMSE: {val_rmse:.4f}")

"""Model menghasilkan RMSE 0.4743 dan loss 0.6689. Nilai RMSE ~0.47 (dalam skala 0-1) menunjukkan kesalahan prediksi cukup signifikan.

### Evaluasi MAE pada Validation set
"""

y_pred = model.predict(x_val).flatten()
mae = mean_absolute_error(y_val, y_pred)
print(f"MAE: {mae:.4f}")

"""MAE 0.4726 (hampir sama dengan RMSE) mengindikasikan kesalahan prediksi cenderung konsisten tanpa banyak outlier."""